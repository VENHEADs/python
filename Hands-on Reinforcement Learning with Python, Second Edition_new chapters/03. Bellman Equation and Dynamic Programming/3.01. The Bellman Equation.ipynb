{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The Bellman Equation\n",
    "\n",
    "The Bellman equation, named after Richard Bellman, helps us to solve the Markov decision process (MDP). When we say solve the MDP, it means finding the optimal policy. \n",
    "\n",
    "The Bellman equation is ubiquitous in reinforcement learning and it is widely used for finding the optimal value function and Q function in a recursive fashion. Computing optimal value and Q function are very important because once we have optimal value or optimal Q function then we can use them to derive the optimal policy. \n",
    "\n",
    "In this section, we learn what exactly is the Bellman equation and how can we use it to find the optimal value and Q function. \n",
    "\n",
    "## Bellman Equation of the value function \n",
    "\n",
    "Bellman equation states that the value of a state can be obtained as a sum of the immediate reward and the discounted value of the next state. Say, we perform an action $a$ and in the state $s$ and move to the next state $s'$ and obtain a reward $r$, then the Bellman equation of the value function can be expressed as:\n",
    "\n",
    "$$V(s) = R(s,a,s') + \\gamma V(s') $$\n",
    "\n",
    "In the above equation, the following applies:\n",
    "\n",
    "* $R(s,a,s')$ implies the immediate reward obtained while performing an action $a$ in the state $s$ and moving to the next state $s$\n",
    "* $\\gamma$ is the discount factor\n",
    "* $V(s')$ implies the value of the next state \n",
    "Let's understand the Bellman equation with an example. Say, we generate a trajectory $\\tau$ using some policy $\\pi$ as shown below:\n",
    "\n",
    "![title](Images/1.PNG)\n",
    "\n",
    "Let's suppose, we need to compute the value of a state $s_2$, then according to Bellman equation the value of a state $s_2$ is given as:\n",
    "\n",
    "$$V(s_2) = R(s_2, a_2,s_3) + \\gamma V(s_3) $$\n",
    "\n",
    "In the above equation, $R(s_2, a_2,s_3) $ implies the immediate reward we obtain while performing an action $a_2$ in the state $s_2$ and moving to the state $s_3$. From the above trajectory, we can tell that the immediate reward $R(s_2, a_2,s_3) $ is $r_2$.  And the term $\\gamma V(s_3) $ implies the discounted value of the next state. \n",
    "\n",
    "\n",
    "Thus, according to the Bellman equation, the value of a state $s_2$ is given as:\n",
    "\n",
    "$$V(s_2) = r_2 + \\gamma V(s_3) $$\n",
    "\n",
    "Thus, the Bellman equation of value function can be expressed as:\n",
    "$$V^{\\pi}(s) = R(s,a,s') + \\gamma V^{\\pi}(s') $$\n",
    "\n",
    "\n",
    "Where the superscript $\\pi$ implies that we are using the policy $\\pi$ and the right-hand side term $$R(s,a,s') + \\gamma V^{\\pi}(s') $$ is often called Bellman backup. \n",
    "\n",
    "\n",
    "\n",
    "The above Bellman equation works only when we have a deterministic environment. Let's suppose our environment is stochastic then in that case when we perform an action  $a$ in the state $s$, it is not guaranteed that our next state will always be $s'$, it could be some other states too. For instance look at the below trajectory, as we can notice, when we perform an action $a_1$ in the state $s_1$, with probability 0.7 we reach the state $s_2$ and with probability 0.3 we reach the state as $s_3$:\n",
    "\n",
    "\n",
    "![title](Images/2.PNG)\n",
    "\n",
    "\n",
    "Thus, when we perform an action $a_1$ in the state $s_1$, the next state will be $s_2$ with 0.7 probability and $s_3$ with 0.3 probability. We learned that the Bellman equation is a sum of immediate reward and the discounted value of the next state. But when our next state is not guaranteed due to the stochasticity present in the environment how can we define our bellman equation?\n",
    "\n",
    "So, in this case, we can slightly modify our Bellman equation with the expectations, that is, weighted average - a sum of Bellman backup multiplied by their corresponding transition probability of next state:\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^{\\pi}(s') ] $$\n",
    "\n",
    "\n",
    "In the above equation, the following applies:\n",
    "\n",
    "* $P(s'|s,a) $ denotes the transition probability of reaching $s'$ by performing an action $a$ in the state $s$\n",
    "* $R(s,a,s') + \\gamma V^{\\pi}(s') $ denotes the Bellman backup\n",
    "\n",
    "\n",
    "\n",
    "Let's understand this equation better by considering the same trajectory we have seen above. As we notice when we perform an action $a_1$ in the state $s_1$, we go to $s_2$ with probability 0.70 and $s_3$  with probability 0.30, thus we can write:\n",
    "\n",
    "$$V(s_1) = P(s_2 |s_1,a_1)[R(s_1,a_1,s_2) + \\gamma V(s_2) ] + P(s_3 |s_1,a_1)[R(s_1,a_1,s_3) + \\gamma V(s_3) ] $$\n",
    "\n",
    "$$V(s_1) = 0.70 [R(s_1,a_1,s_2) + \\gamma V(s_2) ] + 0.30[R(s_1,a_1,s_3) + \\gamma V(s_3) ] $$\n",
    "\n",
    "\n",
    "Thus, the Bellman equation of the value function including the stochasticity present in the environment using the expectation(weighted average) is expressed as:\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^{\\pi}(s') ] $$\n",
    "\n",
    "Okay, but what if when our policy is a stochastic policy? We learned that with stochastic policy, we select actions based on a probability distribution, that is, instead of performing the same action in a state, we select an action based on the probability distribution over action space. Let's understand this with a different trajectory shown below, as we may notice, in the state $s_1$, with probability 0.8 we select action $a_1$ and reach the state $s_2$ and with probability 0.2 we select action  $a_2$ and reach the state $s_3$ :\n",
    "\n",
    "\n",
    "![title](Images/3.png)\n",
    "\n",
    "Thus, when we use stochastic policy, our next state will not always be the same, it will be different states with some probability. Now how can we define the Bellman equation including the stochastic policy? We learned that to include stochasticity present in the environment into the Bellman equation, we took the expectation, that is, weighted average - a sum of bellman backup multiplied by their corresponding transition probability of the next state.\n",
    "\n",
    "Similarly, to include the stochastic nature of policy in the Bellman equation, we can use the expectation, that is, weighted average - a sum of Bellman backup multiplied by their corresponding probability of action. \n",
    "\n",
    "Thus, our final Bellman equation of the value function can be written as:\n",
    "\n",
    "$$V^{\\pi}(s) = \\sum_a \\pi(a|s) \\sum_{s'} P(s'|s,a) [R(s,a,s') + \\gamma V^{\\pi}(s')] -- (1) $$\n",
    "\n",
    "The above equation is also known as the Bellman expectation equation of the value function. We can also express the above equation in expectation form. Let's recollect the definition of expectation:\n",
    "\n",
    "$$\\mathbb{E}_{x \\sim p(x)}[f(X)] = \\sum\\limits_x p(x) f(x) $$\n",
    "\n",
    "\n",
    "In equation (1), $f(x) = R(s,a,s') + \\gamma V^{\\pi}(s') $ and $P(x) = P(s'|s,a) \\,\\, \\text{and} \\,\\,\\pi(a|s) $. \n",
    "\n",
    "Thus, we can write the Bellman equation of value function as:\n",
    "\n",
    "$$V^{\\pi}(s)=\\underset{\\begin{align} a \\sim \\pi \\\\ s' \\sim P\\end{align}}{\\mathbb{E}}\\left[R(s, a,s')+\\gamma V^{\\pi}\\left(s^{\\prime}\\right)\\right]$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## Bellman Equation of the Q function \n",
    "\n",
    "Now, we will learn how to compute the Bellman equation of the state-action value function, that is, Q function. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To learn more, please check the section Bellman Equation of the Q function in the book. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
