{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classic control environments\n",
    "\n",
    "The gym provides environments for several classic control tasks such as cart pole balancing, swinging up the pendulum, mountain car climbing and so on. Let's understand how to create a gym environment for a cart pole balancing task. The cart pole environment is shown below:\n",
    "\n",
    "\n",
    "![title](Images/17.PNG)\n",
    "\n",
    "Cart Pole balancing is one of the classical control problems. As shown in the above figure, the pole is attached to the cart and the goal of our agent is to balance the pole on the cart, that is, the goal of our agent is to keep the pole straight up standing on the cart as shown below:\n",
    "\n",
    "![title](Images/18.PNG)\n",
    "\n",
    "\n",
    "So the agent tries to push the cart left and right to keep the pole standing straight on the cart. Thus our agent performs two actions which are pushing the cart to the left and pushing the cart to the right to keep the pole standing straight on the cart. You can also check this very interesting video https://youtu.be/qMlcsc43-lg which shows how the RL agent balances the pole on the cart by moving the cart left and right. \n",
    "\n",
    "Now, let's learn how to create the cart pole environment using the gym. The environment id of the cart pole environment in the gym is `CartPole-v0` , so we can just use our `make` function to create the cart pole environment as shown below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "env = gym.make(\"CartPole-v0\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After creating, we can view our environment using the `render` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.render()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also close rendering the environment using the `close` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State space\n",
    "\n",
    "Now, let's look at the state space of our cart pole environment. Wait! What are the states here? In the frozen lake environment we had discrete 16 states from (S to G). But how can we describe the states here? Can we describe the state by cart position? Yes! Note that the cart position is a continuous value. So, in this case, our state space will be continuous values, unlike the frozen lake environment where our state space had discrete values (S to G).\n",
    "\n",
    "But with just the cart position alone we cannot describe the state of the environment completely. So we include cart velocity, pole angle and pole velocity at the tip. So we can describe our state space by an array of values as shown below:\n",
    "\n",
    "`array([cart position, cart velocity, pole angle, pole velocity at the tip])`\n",
    "\n",
    "Note that all of these values are continuous, that is:\n",
    "\n",
    "* The value of cart position ranges from -4.8 to 4.8\n",
    "* The value of cart velocity ranges from -Inf to Inf\n",
    "* The value of pole angle ranges from -0.418 radians to 0.418 radians \n",
    "* The value of pole velocity city at the tip ranges from -Inf to Inf\n",
    "\n",
    "Thus, our state space contains an array of continuous values. Let's learn how can we obtain this from the gym. In order to get the state space, we can just type `env.observation_space` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Box(4,)\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Box implies that our state space consists of continuous values and not discrete values. That is, in the frozen lake environment, we obtained the state space as `Discrete(16)` which implies that we have 16 discrete states (S to G). But now we got our state space as `Box(4,)` which implies that our state space is continuous and consists of an array of 4 values.\n",
    "\n",
    "For example, let's reset our environment and see how our initial state space will look like. We can reset the environment using the `reset` function:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.0468521   0.04980211 -0.0063804   0.04013309]\n"
     ]
    }
   ],
   "source": [
    "print(env.reset())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It implies our initial state space, as we can notice, we have an array of 4 values which denotes the cart position, cart velocity, pole angle and pole velocity at the tip respectively. That is:\n",
    "\n",
    "![title](Images/19.PNG)\n",
    "\n",
    "Okay, how can we obtain the maximum and minimum values of our state space? We can obtain the maximum values of our state space using `env.observation_space.high` and the minimum values of our state space using `env.observation_space.low`\n",
    "\n",
    "For example, let's look at the maximum value of our state space:\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8000002e+00 3.4028235e+38 4.1887903e-01 3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.high)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It implies that:\n",
    "\n",
    "1. The maximum value of the cart position is 4.8\n",
    "2. We learned that the maximum value of cart velocity is  +Inf, we know that infinity is not really a number, so it is represented using the largest positive real value 3.4028235e+38.\n",
    "3. The maximum value of the pole angle is 0.418 radians.\n",
    "4. The maximum value of pole velocity at the tip is +Inf, so it is represented using largest positive real value 3.4028235e+38\n",
    "\n",
    "Similarly, we can obtain the minimum value of our state space as:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-4.8000002e+00 -3.4028235e+38 -4.1887903e-01 -3.4028235e+38]\n"
     ]
    }
   ],
   "source": [
    "print(env.observation_space.low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action space\n",
    "\n",
    "Now, let's look at the action space. We already learned that in the cart pole environment we perform two actions which are pushing the cart to the left and pushing the cart to the right and thus action space is discrete since we have only two discrete actions.\n",
    "\n",
    "In order to get the action space, we can just type `env.action_space` as shown below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Discrete(2)\n"
     ]
    }
   ],
   "source": [
    "print(env.action_space)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can observe `Discrete(2)` implies that our action space is discrete and we have two actions in our action space. Note that the actions will be encoded into numbers as shown below:\n",
    "\n",
    "\n",
    "![title](Images/20.PNG)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
