{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Markov Decision Processes \n",
    "\n",
    "The Markov Decision Process (MDP) provides a mathematical framework for solving the reinforcement learning (RL) problem. Almost all RL problems can be modeled as MDP. MDP is widely used for solving various optimization problems. In this section, we will understand what is MDP and how it is used in reinforcement learning. \n",
    "\n",
    "First, let's understand what is MDP and then we will learn how it is used in reinforcement learning. In order to understand MDP, first, we learn the Markov property and Markov chain.\n",
    "\n",
    "## Markov Property and Markov Chain \n",
    "\n",
    "The Markov property states that the future depends only on the present and not on the past. The Markov chain also know as the Markov process consists of a sequence of states which strictly obeys the Markov property, that is, Markov chain is the probabilistic model that solely depends on the current state to predict the next state and not the previous states, that is, the future is conditionally independent of the past.\n",
    "\n",
    "For example, if we know that the current state is cloudy, we can predict that the next state could be rainy. We came to this conclusion that the next state could be rainy only by considering the current state (cloudy) and not the previous states, which might be sunny, windy, and so on. However, the Markov property does not hold true for all processes. For instance, throwing a dice (the next state) has no dependency on the previous number whatever showed up on the dice (the current state).\n",
    "\n",
    "Moving from one state to another is called transition and its probability is called a transition probability. We denote the transition probability by $P(s'|s) $. It indicates the probability of moving from the state $s$ to the next state $s'$.  Say, we have three states, cloudy, rainy and windy in our Markov chain. Then we can represent the probability of transitioning from one state to another using a table called Markov table as shown below:\n",
    "\n",
    "![title](Images/8.PNG)\n",
    "\n",
    "From the above Markov table, we can observe that:\n",
    "\n",
    "From the state cloudy we transition to the state rainy with 70% probability and to the state windy with 30% probability.\n",
    "From the state rainy we transition to the same state rainy with 80% probability and to the state cloudy with 20% probability\n",
    "From the state windy, we transition to the state rainy with 100% probability.\n",
    "We can also represent this transition information of the Markov chain in the form of a state diagram as shown below:\n",
    "\n",
    "![title](Images/9.png)\n",
    "\n",
    "We can also formulate the transition probabilities into a matrix called the transition matrix as shown below:\n",
    "\n",
    "![title](Images/10.PNG)\n",
    "\n",
    "Thus, to conclude we can say that the Markov chain or Markov process consists of a set of states along with their transition probabilities.\n",
    "\n",
    "## Markov Reward Process\n",
    "Markov Reward Process (MRP) is an extension of the Markov chain with the reward function. That is, we learned that the Markov chain consists of states and transition probability. Now the Markov reward process consists of states, transition probability and also reward function.\n",
    "\n",
    "Reward function tells us what is the reward we obtain in each state. For instance, what is the reward we obtain in the state cloudy, what is the reward we obtain in the state windy and so on. The reward function is usually denoted by $R(s)$. \n",
    "\n",
    "Thus the Markov Reward Process (MRP) consists of states $s$, transition probability $P(s'|s) $ and reward function $R(s)$. \n",
    "\n",
    "## Markov Decision Process\n",
    "Markov Decision Process (MDP) is an extension of the Markov reward process with actions. That is, we learned that the Markov reward process consists of states, transition probability, and the reward function. Now the Markov decision process consists of states, transition probability, reward function and also actions. Let's understand MDP clearly by relating to the reinforcement learning environment. \n",
    "\n",
    "Okay, the question is how the Markov decision process is used in reinforcement learning? We learned that Markov property states that the next state is dependent only on the current state and not based on the previous state. Does the Markov property applicable to the reinforcement learning setting? Yes! since in the reinforcement learning environment, the agent makes decisions only based on the current state and not the past states. So, we can model the reinforcement learning environment as the Markov decision process.\n",
    "\n",
    "Let's understand this with an example. Given any environment, we can formulate the environment using a Markov decision process. For instance, let's consider the same grid world environment we learned earlier. The grid world environment is shown below and the goal of the agent is to reach the state I from state A without visiting the shaded states:\n",
    "\n",
    "\n",
    "![title](Images/11.png)\n",
    "\n",
    "An agent makes a decision(action) in the environment only based on the current state where the agent is in and not based on the past state. So we can formulate our environment as a Markov decision process. We learned that the MDP consists of states, actions, transition probability, and reward function. Now let's learn how does it relate to our reinforcement learning environment. \n",
    "\n",
    "__States__ -  A set of states present in the environment. Thus, in the grid world environment, we have states A to I.\n",
    "\n",
    "__Actions__ - A set of actions that our agent can perform in each state. An agent performs an action and moves from one state to another. Thus, in the grid world environment, the set of actions includes up, down, left and right. \n",
    "\n",
    "__Transition probability__ - The transition probability is denoted by $ P(s'|s,a) $. It implies the probability of moving from a state $s$ to the next state $s'$ while performing an action $a$. If you observe, in the Markov reward process (MRP), the transition probability is just $P(s'|s)$, that is, probability of going from state $s$ to state  $s'$ and it doesn't include actions. But in MDP we include the actions, thus the transition probability is denoted by $ P(s'|s,a) $. \n",
    "\n",
    "For example, in our grid world environment, say, the transition probability of moving from state A to state B while performing an action right is 100% then it can be expressed as: $P( B |A , \\text{right}) = 1.0 $. We can also view this in the state diagram as shown below:\n",
    "\n",
    "\n",
    "![title](Images/12.png)\n",
    "\n",
    "Suppose, our agent is in state C and the transition probability of moving from state C to the state F while performing an action down is 90% then it can be expressed as: $P( C |F , \\text{down}) = 0.9 $. We can also view this in the state diagram as shown below:\n",
    "\n",
    "\n",
    "![title](Images/13.png)\n",
    "\n",
    "__Reward function__ -  The reward function is denoted by $R(s,a,s') $. It implies the reward our agent obtains while transitioning from a state $s$ to the state $s'$ while performing an action $a$. \n",
    "\n",
    "Say, the reward we obtain while transitioning from the state A to the state B while performing an action right is -1, then it can be expressed as $R(A, \\text{right}, B) = -1 $. We can also view this in the state diagram as shown below:\n",
    "\n",
    "\n",
    "![title](Images/14.png)\n",
    "\n",
    "Suppose, our agent is in state C and say, the reward we obtain while transitioning from the state C to the state F while performing an action down is  +1, then it can be expressed as $R(C, \\text{down}, F) = +1 $. We can also view this in the state diagram as shown below:\n",
    "\n",
    "\n",
    "![title](Images/15.png)\n",
    "\n",
    "Thus, a reinforcement learning environment can be represented as a Markov decision process with states, actions, transition probability, and the reward function. But wait! What is this use of representing the reinforcement learning environment using the MDP? We can solve the reinforcement learning problem easily once we model our environment as MDP. For instance, once we model our grid world environment using the MDP then we can easily find how to reach the goal state I from state A without visiting the shaded states. We will learn more about this in the upcoming chapters. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
