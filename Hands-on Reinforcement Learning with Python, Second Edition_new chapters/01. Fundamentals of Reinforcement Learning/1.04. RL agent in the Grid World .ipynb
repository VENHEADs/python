{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RL agent in the Grid World \n",
    "\n",
    "Let's strengthen our understanding of reinforcement learning by looking at another interesting example. Consider a grid world environment shown below:\n",
    "\n",
    "![title](Images/4.png)\n",
    "\n",
    "The positions A to I in the environment are called the states of the environment. The goal of the agent is to reach the state I from starting the state A without visiting the shaded states (B, C, G, and H). Thus, in order to achieve the goal, whenever our agent visits the shaded state, we will give a negative reward (say -1) and when it visits the unshaded state, we will give a positive reward (say +1). The actions in the environment include - up, down, right and left. The agent performs any of these four actions to reach the state I from state A. \n",
    "\n",
    "Note that the above our version of the grid world environment is slightly different from the common grid world environments. In general, the shaded states are treated as obstacles and whenever the agent visits the shaded states it has to go back to the starting state (A) and start the game again. \n",
    "\n",
    "But in our version of the grid world environment shown above, the shaded states are NOT the obstacle, it is simply a bad state. So, we assign -1 reward if the agent visits the shaded state and +1 reward if the agent visits the unshaded state. The goal of the agent is to learn to reach the state I from A without visiting the shaded states. Let's now understand how the agent learns to achieve this goal. \n",
    "\n",
    "In the first iteration, that is, the first time the agent interacts with the environment, the agent cannot perform the correct action in each state and thus it receives a negative reward. That is, in the first iteration, the agent performs a random action in each state and so it may receive a negative reward. But over a series of iterations, the agent learns to perform the correct action in each state through the reward it obtains and achieve the goal. \n",
    "\n",
    "## Iteration 1:\n",
    "\n",
    "In the first iteration, agent performs random action in each state, for instance, look at the figure shown below, in the first iteration the agent performs the action right in state A and moves to a new state B. But since B is the shaded state, the agent will receive a negative reward and so the agent will understand that right is not a good action in the state A and when it visits the state A next time it will try out different action instead of right. \n",
    "\n",
    "![title](Images/5.PNG)\n",
    "\n",
    "As shown in the above figure, from state B, the agent performs action down and moves to the new state E and since E is an unshaded state the agent will receive a positive reward, so the agent will understand that performing action down in the state B is a good action.\n",
    "\n",
    "From the state E, the agent performs the action right and reaches the state F since F is an unshaded state agent receives a positive reward and it will understand that performing right in the state E is a good action. From the state F, the agent performs the action down and reaches the goal state I and receives a positive reward so the agent will understand that performing action down in the state F is a good action:\n",
    "\n",
    "## Iteration 2:\n",
    "\n",
    "In the second iteration, in state A, instead of performing action right, agent try out different action as the agent learned in the previous iteration that right is not a good action in the state A. \n",
    "\n",
    "Thus, as shown in the below figure, in this iteration, in state A, the agent tries out different action, say the action down. So the agent performs action down in the state A and reaches the state D. Since D is an unshaded state, our agent receives a positive reward and so the agent will understand that the action down is a good action in the state A. \n",
    "\n",
    "![title](Images/6.PNG)\n",
    "\n",
    "As shown in the above figure, from the state D, the agent performs an action down and reaches the state G. But since G is a shaded state the agent will receive a negative reward and so the agent will understand that down is not a good action in the state D and when it visits the state D next time it will try out different action instead of down. \n",
    "\n",
    "From G it performs action right and reaches H since H is a shaded state it will receive a negative reward and understand that right is not a good action in the state G and from H it performs an action right and reaches the goal state I and receives a positive reward so the agent will understand that performing action right in the state H is a good action. \n",
    "\n",
    "## Iteration 3:\n",
    "\n",
    "In the third iteration, the agent performs action down in state A since, in the second iteration, our agent learned that performing action down is a good action in state A. So, the agent performs action down in state A and reaches the next state D as shown in the below figure. \n",
    "\n",
    "![title](Images/7.PNG)\n",
    "\n",
    "Now from the state D agent tries different action instead of action down since in the second iteration our agent learned that performing action down is not a good action in the state D. So, in this iteration, our agent tries a different action in the state say right. \n",
    "\n",
    "The agent performs action right in the state D and moves to the new state E. From the state E agent performs action right as the agent already learned in the first iteration that performing action right in the state E is a good action. So from the state E the agent performs action right and reaches the state F.\n",
    "\n",
    "Now from the state F agent perform action down since the agent learned in the first iteration that performing down is a good action in the state F. So from the state F the agent performs action down and reaches the goal state I.\n",
    "\n",
    "Thus, this is the result of the third iteration: \n",
    "![title](Images/7.PNG)\n",
    "\n",
    "\n",
    "As we can notice, our agent has successfully learned to reach the state I from state A without visiting the shaded states based on the rewards. \n",
    "\n",
    "In this way, the agent will try out different actions in each state and understand whether an action is good or bad based on the reward it obtains. The goal of the agent is to maximize rewards. So, the agent will always try to perform good actions which give a positive reward and when the agent performs good actions in each state then it ultimately leads the agent to achieve the goal. \n",
    "\n",
    "Note that these iterations are basically called the episodes in the reinforcement learning terminology. We will learn more about the episode in the upcoming section."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
