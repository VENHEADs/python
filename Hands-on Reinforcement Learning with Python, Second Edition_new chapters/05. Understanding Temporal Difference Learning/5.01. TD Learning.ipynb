{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TD learning\n",
    "\n",
    "The Temporal Difference (TD) learning algorithm was introduced by Richard S. Sutton in 1988. In the introduction of the chapter, we learned that the reason for the TD method to be more popular is that it takes the advantages of the dynamic programming method and the Monte Carlo method. But what are those advantages?\n",
    "\n",
    "First, let's recap quickly the advantages and disadvantages of dynamic programming and the Monte Carlo method.\n",
    "\n",
    "**Dynamic Programming** - The advantage of the dynamic programming (DP) method is that it uses the Bellman equation to compute the value of a state. That is, we learned that according to the Bellman equation, the value of a state can be obtained as a sum of the immediate reward and the discounted value of the next state. This is called bootstrapping. That is, to compute the value of a state, we don't have to wait till the end of the episode, instead, using the Bellman equation, we can estimate the value of a state just based on the value of the next state and this is called bootstrapping.\n",
    "\n",
    "Remember how we estimated the value function in dynamic programming methods (value and policy iteration)? We estimated the value function (value of a state) as $V(s) = \\sum_{s'} P_{ss'}^a [R_{ss'}^a + \\gamma  V^{}(s')] $ .  As you may recollect, we learned that in order to find the value of a state, we didn't have to wait till the end of the episode, instead, we bootstrap, that is we estimate the value of the current state $V(s)$ by estimating the value of the next state $V(s')$. \n",
    "\n",
    "However, the disadvantage of dynamic programming is that we can apply the DP method only when we know the model dynamics of the environment. That is, we learned that the DP is a model-based method and we should know the transition probabilities in order to use the DP method. When we don't know the model dynamics of the environment then we cannot apply the DP method. \n",
    "\n",
    "**Monte Carlo Method** - The advantage of the Monte Carlo method is that is it is a model-free method which means that it does not require the model dynamics of the environment to be known in order to estimate the value and Q function. \n",
    "\n",
    "However, the disadvantage of the Monte Carlo method is that in order to estimate the state value or Q value we need to wait till the end of the episode and if the episode is long then it will cost us a lot of time. Also, we cannot apply Monte Carlo methods to continuous tasks (non-episodic tasks). \n",
    "\n",
    "\n",
    "Now let's get back to TD learning. The TD learning algorithm takes the benefits of both the dynamic programming and the Monte Carlo methods into account. That is just like a dynamic programming method, we perform bootstrap so that we don't have to wait till the end of an episode to compute the state value or Q value and just like the Monte Carlo method, it is a model-free method and so it does not require the model dynamics of the environment to compute the state value or Q value. Now that we have understood the basic idea behind the TD learning algorithm, let's get into detail and learn how exactly it works. \n",
    "\n",
    "Similar to what we learned in the Monte Carlo chapter, we can use the TD learning algorithm for both the prediction and control tasks and so we can categorize TD learning into:\n",
    "\n",
    "* TD Prediction \n",
    "* TD Control \n",
    "\n",
    "We already learned what does prediction and control method means in the previous chapter. Let us recap that a bit before going forward. \n",
    "\n",
    "In the prediction method, a policy is given as an input and we try to predict the value function or Q function using the given policy. If we predict the value function using the given policy then we can say how good it is for the agent to be in each state if it uses the given policy. That is, we can say that what is the expected return an agent can get in each state if it acts according to the given policy. \n",
    "\n",
    "In the Control method, we will not be given any policy as an input and the goal in the control method is to find the optimal policy. So, we will initialize a random policy and then we try to find the optimal policy iteratively. That is, we try to find an optimal policy that gives the maximum return. \n",
    "\n",
    "First, let us understand how can we use TD learning to perform prediction task and then we will learn how to use TD learning for the control task."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
