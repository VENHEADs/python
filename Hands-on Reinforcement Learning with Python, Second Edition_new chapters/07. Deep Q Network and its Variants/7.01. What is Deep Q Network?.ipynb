{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# What is Deep Q Network?\n",
    "\n",
    "The objective of reinforcement learning is to find the optimal policy, that is, the policy which gives the maximum return (sum of rewards of the trajectory). In order to compute the policy, first, we compute the Q function. Once we have the Q function then we extract policy by selecting an action in each state which has the maximum Q value. For instance, let's suppose we have two states A and B and our action space consists of two actions, let the actions be up and down. So, in order to find which action to perform in state A and B, first, we compute the Q value of all state-action pairs as shown below:\n",
    "\n",
    "\n",
    "![title](Images/1.PNG)\n",
    "\n",
    "Once we have the Q value of all state-action pairs then we select the action in each state which has the maximum Q value. So, we select action up in state A and down in state B as they have the maximum Q value. We improve the Q function on every iteration and once we have the optimal Q function then we can extract the optimal policy from it. \n",
    "\n",
    "Now, let's revisit our grid world environment as shown below:\n",
    "\n",
    "\n",
    "\n",
    "![title](Images/2.PNG)\n",
    "\n",
    "We learned that in the grid world environment, the goal of our agent is to reach the state I from the state A without visiting the shaded states and in each state, the agent has to perform any of the four actions - up, down, left, right. \n",
    "\n",
    "To compute the policy, first, we compute the Q value of all state-action pairs. Here, the number of states is 9 (A to I)  and we have 4 actions in our action space, so our Q table will consist of 9 x 4 = 36 rows containing Q value of all possible state-action pairs. Once we obtain this Q value then we extract policy by selecting an action in each state which has the maximum Q value. But is it a good approach to compute the Q value exhaustively for all state-action pairs? Let's explore this in more detail.  \n",
    "\n",
    "Let's suppose an environment where we have 1000 states and 50 possible actions in each state, in this case, our Q table will consist of 1000 x 50 = 50000 rows containing Q values of all possible state-action pairs and cases like this where our environment consists of a large number of states and actions, it will be very expensive to compute Q value of all possible state-action pairs in an exhaustive fashion.\n",
    "\n",
    "Instead of computing Q value in this way, can we approximate them using any function approximator, say neural network? That is, can we approximate the Q value using a neural network? Yes! We can parameterize our Q function by some parameter $\\theta$ and compute the Q value where the parameter $\\theta$ is just the parameter of our neural network. So, we just feed the state of the environment to a neural network and it will return the Q value of all possible actions in that state. Once we obtain the Q values, then we can select the best action as the one which has the maximum Q value.\n",
    "\n",
    "For example, let's consider our grid world environment, as shown below, we just feed the state D as an input to the network and it returns the Q value of all actions in the state D which are up, down, left and right as an output. Then, we select action as the one which has the maximum Q value. Since action right has a maximum Q value, we select action right in the state D:\n",
    "\n",
    "\n",
    "![title](Images/3.png)\n",
    "\n",
    "Since we are using the neural network to approximate the Q value, the neural network is called the Q network and if we use the deep neural network to approximate the Q value then the deep neural network is called the deep Q network (DQN).\n",
    "\n",
    "\n",
    "We can denote our Q function by $Q_{\\theta}(s,a) $ where the parameter $\\theta$ on the subscript indicates that our Q function is parameterized by $\\theta$ and $\\theta$ is just the parameter of our neural network. \n",
    "\n",
    "\n",
    "We initialize the parameter $\\theta$ of the network with random values and approximate the Q function (Q values), but since we initialized $\\theta$ with random values, the approximated Q function will not be optimal. So we train the network for several iterations by finding the optimal parmater $\\theta$ , once we find the optimal $\\theta$, we will have the optimal Q function. Then we can extract optimal policy from the optimal Q function.\n",
    "\n",
    "Okay, but how can we train our network? what about the training data and the loss function? Is it a classification or regression task? Now that we have a basic understanding of how DQN works, in the next section, we will get into details and address all these questions. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
